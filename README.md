# Synthesis Labs

Cross-domain research hypothesis generation using AI agents.

## Overview

Synthesis Labs finds the research worth doing that humans wouldn't have thought to try. It works by:

1. **Analyzing** a research domain to extract concepts, methods, and open problems
2. **Cross-pollinating** to find analogous problems and methods in other domains
3. **Synthesizing** novel research hypotheses from these connections
4. **Validating** hypotheses for specificity, novelty, and feasibility

## Quick Start

```bash
# Install dependencies
npm install

# Set your API key (choose one method)

# Option 1: Create .env file (recommended)
cp .env.example .env
# Edit .env and add your key: ANTHROPIC_API_KEY=sk-ant-...

# Option 2: Environment variable
# PowerShell: $env:ANTHROPIC_API_KEY="sk-ant-..."
# CMD:        set ANTHROPIC_API_KEY=sk-ant-...
# macOS/Linux: export ANTHROPIC_API_KEY=sk-ant-...

# Run a synthesis
npm run dev -- "How can we improve CRISPR guide RNA design?"
```

## Usage

```bash
# Basic usage
synth "Your research question here"

# Specify target domain
synth "Question" --domain=computational-biology

# Enable trace output (saves detailed LLM inputs/outputs)
synth "Question" --trace --trace-dir=./my-traces

# Set token budget (fails if exceeded)
synth "Question" --max-tokens=50000

# All options
synth "Question" --domain=ml-ai --trace --max-tokens=100000
```

### Available Options

| Option | Description |
|--------|-------------|
| `--domain=<domain>` | Target domain: `computational-biology`, `materials-science`, `ml-ai` |
| `--trace` | Enable detailed trace output to disk |
| `--trace-dir=<path>` | Trace output directory (default: `./traces`) |
| `--max-tokens=<num>` | Token budget cap (fails if exceeded) |

## Example Output

```
========================================
  SYNTHESIS LABS
  Cross-Domain Research Hypothesis Generation
========================================

Query: "How can we improve CRISPR & guide RNA design?"
Domain: Computational Biology

SUMMARY:
  Generated: 5 hypotheses
  Validated: 3
  Rejected: 2
  Time: 45.2s

TOKEN USAGE:
  Input tokens: 12,345
  Output tokens: 3,456
  Total tokens: 15,801
  Estimated cost: $0.2347

PIPELINE STAGES:
  ✓ domain-analysis: 8.3s
  ✓ cross-pollination: 12.1s
  ✓ hypothesis-synthesis: 15.4s
  ✓ hypothesis-challenge: 9.4s

----------------------------------------
  VALIDATED HYPOTHESES
----------------------------------------

#1: Attention-Based CRISPR Off-Target Prediction
Verdict: PASS (Score: 3.65)

Statement: Transformer attention mechanisms can improve CRISPR
off-target prediction accuracy by learning position-dependent
nucleotide binding patterns.

Scores:
  Specificity: 4/5
  Novelty: 3/5
  Connection Validity: 4/5
  Feasibility: 4/5
  Grounding: 3/5
...
```

## How It Works

### Pipeline Architecture

```
User Query
    │
    ▼
┌─────────────────┐
│ Domain Analyst  │ → Extracts concepts, methods, problems
└─────────────────┘
    │
    ▼
┌─────────────────┐
│ Cross-Pollinator│ → Finds analogies in other domains
└─────────────────┘
    │
    ▼
┌─────────────────┐
│ Hypothesis      │ → Generates candidate hypotheses
│ Synthesizer     │
└─────────────────┘
    │
    ▼
┌─────────────────┐
│ Hypothesis      │ → Validates and scores hypotheses
│ Challenger      │
└─────────────────┘
    │
    ▼
Ranked Output
```

### Hypothesis Quality Rubric

Each hypothesis is scored on 5 dimensions:

| Dimension | Weight | Description |
|-----------|--------|-------------|
| Specificity | 25% | Is it testable with clear variables? |
| Novelty | 20% | Is it genuinely new? |
| Connection Validity | 25% | Is the cross-domain analogy genuine? |
| Feasibility | 15% | Can it be tested with reasonable resources? |
| Grounding | 15% | Is there supporting evidence? |

**Pass threshold**: Composite score ≥ 3.5, no dimension below 2

## Observability

### Stage Timing

Each stage's duration depends on query complexity and LLM response time:

| Stage | Typical Duration | What It Does |
|-------|-----------------|--------------|
| domain-analysis | 5-15s | Extracts concepts, methods, open problems |
| cross-pollination | 10-20s | Finds analogous patterns across domains |
| hypothesis-synthesis | 10-25s | Generates candidate hypotheses |
| hypothesis-challenge | 8-15s | Scores and validates hypotheses |

### Token Tracking

Every run displays token usage and estimated cost:
- **Input tokens**: Tokens sent to the model (prompts)
- **Output tokens**: Tokens generated by the model
- **Estimated cost**: USD cost based on Claude pricing

Token usage is tracked per-agent and per-model internally. See `metadata.json` in trace output for full breakdown.

### Trace Output

Use `--trace` to save detailed LLM inputs/outputs for debugging:

```bash
synth "Your query" --trace --trace-dir=./traces
```

Creates a trace directory with:
```
traces/{trace-id}/
├── metadata.json              # Run metadata, totals, cost
├── stage-01-domain-analyst.json
├── stage-02-cross-pollinator.json
├── stage-03-hypothesis-synthesizer.json
└── stage-04-hypothesis-challenger.json
```

Each stage file contains:
- `input.system` — exact system prompt sent to LLM
- `input.user` — exact user prompt sent to LLM
- `output.raw` — full LLM response text
- `usage.inputTokens` / `usage.outputTokens`
- `durationMs` — time for that LLM call

### Budget Enforcement

Use `--max-tokens` to cap token usage:

```bash
synth "Your query" --max-tokens=50000
```

The pipeline will abort with a `BudgetExceededError` if the budget is exceeded.

### Debugging and Optimization

#### Inspecting Traces

After a run with `--trace`, inspect the output:

```bash
# View exact prompts sent to domain analyst
cat traces/{traceId}/stage-01-domain-analyst.json | jq '.input'

# Check token usage per stage
for f in traces/{traceId}/stage-*.json; do
  echo "$(basename $f): $(cat $f | jq '.usage')"
done

# View total cost and timing
cat traces/{traceId}/metadata.json | jq '{cost: .costUsd, start: .startTime, end: .endTime}'
```

#### What to Look For

| Symptom | Check | Location |
|---------|-------|----------|
| Slow stages | `durationMs` in trace files | `stage-*.json` |
| High token usage | Prompt length in `input.system` | `stage-*.json` |
| Poor quality output | Raw LLM response | `output.raw` in trace |
| Retries/failures | Circuit breaker state | Console output |

#### Optimization Levers

- **Reduce cost**: Use `claude-sonnet-4` instead of `claude-opus-4` (configure in orchestrator)
- **Fail fast**: Set `--max-tokens` budget to abort runaway queries
- **Reduce latency**: Adjust timeout/retry settings in orchestrator config
- **Debug prompts**: Inspect `input.system` in traces to optimize prompt verbosity

#### Resilience Features

The pipeline includes built-in resilience:

- **Timeout protection**: 120s default per LLM call (configurable via `timeoutMs`)
- **Retry with backoff**: Exponential backoff on transient failures (default 2 retries)
- **Circuit breaker**: Fails fast after repeated failures (configurable threshold)

## Project Structure

```
synth-research/
├── src/
│   ├── types/           # Type definitions (Zod schemas)
│   ├── agents/          # Agent implementations
│   ├── orchestrator/    # Pipeline coordination
│   ├── context/         # Pipeline context management
│   ├── tracing/         # Trace output writer
│   └── cli.ts           # Command-line interface
├── tests/               # Test suites
├── examples/            # Example runs
├── docs/
│   ├── architecture/    # ADRs
│   └── specs/           # PRD, constraints
└── .claude/
    ├── agents/          # Agent specifications
    └── context/         # Project context
```

## Development

```bash
# Install dependencies
npm install

# Type check
npm run lint

# Run tests
npm test

# Build
npm run build

# Run in development
npm run dev -- "Your query"
```

## Supported Domains

| Domain | Key Areas |
|--------|-----------|
| **Computational Biology** | Genomics, proteomics, drug discovery, CRISPR, systems biology |
| **Materials Science** | Nanomaterials, polymers, semiconductors, biomaterials |
| **ML/AI** | Deep learning, reinforcement learning, NLP, computer vision |

## Limitations

- **Citation verification**: Citations marked as `llm-knowledge` should be independently verified. Automated citation verification is planned for v1.1.
- **Novelty checking**: The system cannot guarantee a hypothesis hasn't been published
- **Domain coverage**: Currently limited to 3 domains
- **No live experiments**: Generates hypotheses only, cannot execute research

## Architecture Decisions

See `docs/architecture/` for detailed ADRs:
- ADR-001: System Overview
- ADR-002: Agent Topology
- ADR-003: Knowledge Representation
- ADR-004: Hypothesis Scoring
- ADR-005: Timeout and Circuit Breaker Strategy (Proposed)
- ADR-006: Parallel Hypothesis Evaluation (Proposed)
- ADR-007: Evidence Gatherer Scope (Proposed)

## License

MIT
