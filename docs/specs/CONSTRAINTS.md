# Synthesis Labs - Context Constraints Document

**Generated by**: project-guardian
**Source SD**: docs/specs/STRATEGY.md
**Date**: 2026-01-16
**Analysis Tier**: Tier 2 (Standard)

---

## Summary

| Category | Count |
|----------|-------|
| **Constraint categories analyzed** | 5 |
| **Blocking constraints** | 2 |
| **Significant constraints** | 6 |
| **Notable constraints** | 4 |
| **Conflicts identified** | 1 |
| **Gaps requiring decision** | 4 |
| **Overall risk level** | MEDIUM |

---

## Blocking Constraints

These must be resolved before proceeding.

### [BLOCK-001] LLM Context Window Limits Cross-Domain Synthesis Depth

**Category**: Technical
**Severity**: BLOCKING

**Description**:
Cross-domain synthesis requires loading context from multiple domains simultaneously. Claude context window (200K tokens) must accommodate:
- Source domain analysis (concepts, methods, problems)
- Target domain context (candidate analogies)
- Paper excerpts/citations for grounding
- Reasoning chains for synthesis
- Output formatting

If synthesis requires more context than available, hypotheses will be shallow or disconnected.

**Hard Requirements**:
- Single synthesis pass must fit within 200K token context
- Must design agent handoffs to minimize redundant context
- Cannot load full papers; must use excerpts/summaries

**Specific Limits** (Claude API as of 2026-01):

| Limit | Value | Impact |
|-------|-------|--------|
| Context window | 200K tokens | Bounds synthesis complexity |
| Output tokens | 8K tokens (default), 64K (extended) | Limits hypothesis detail |
| Rate limit (free tier) | 1 RPM | Unacceptable for development |
| Rate limit (pro tier) | 50 RPM | Sufficient for MVP |
| Cost per 1M input tokens | USD 3.00 (Sonnet), USD 15.00 (Opus) | Budget constraint |
| Cost per 1M output tokens | USD 15.00 (Sonnet), USD 75.00 (Opus) | Budget constraint |

**Owner**:
- Technical Lead (architecture decisions)
- Anthropic API (hard limits)

**Flexibility**: NONE for context window. Rate limits can be increased with higher tier.

**Timeline Impact**:
- Must design context management strategy BEFORE agent implementation
- Adds 2-4 hours to architecture phase

**Resolution Path**:
1. Design tiered context loading (minimal then detailed as needed)
2. Use summarization agents to compress domain context
3. Implement context budget tracking per synthesis
4. Consider multi-turn synthesis for complex hypotheses

---

### [BLOCK-002] Citation Traceability is Non-Negotiable

**Category**: Business / Academic Integrity
**Severity**: BLOCKING

**Description**:
Strategy Document requires all cross-domain connections to be traceable to sources. This is both:
- A user requirement (researchers need citations)
- An ethical requirement (cannot fabricate research connections)

Generated hypotheses that cannot cite supporting evidence are INVALID outputs.

**Hard Requirements**:
- Every claimed cross-domain connection must reference at least one source
- Sources must be verifiable (real papers, not hallucinated)
- Citation format must be usable in academic context

**Owner**:
- Product team (requirement)
- Academic integrity standards (ethical constraint)

**Flexibility**: NONE - core value proposition depends on this

**Timeline Impact**:
- Evidence-gatherer agent is REQUIRED, not optional
- Adds verification pass to hypothesis pipeline
- May slow hypothesis generation by 2-3x

**Resolution Path**:
1. Implement citation verification in evidence-gatherer
2. Use web search to validate paper existence
3. Flag hypotheses with unverifiable citations
4. Output confidence scores based on citation quality

---

## Significant Constraints

These must be addressed in planning.

### [SIG-001] Knowledge Cutoff Limits Novelty Verification

**Category**: Technical
**Severity**: SIGNIFICANT

**Description**:
LLM training data has a cutoff date. Research published after cutoff is unknown to the model. This affects:
- Novelty checking (may claim hypothesis is novel when it was published post-cutoff)
- Cross-domain connections (may miss recent breakthroughs)
- Method recency (may suggest outdated approaches)

**Hard Requirements**:
- Must supplement LLM knowledge with web search for recency
- Cannot rely solely on LLM for novelty verification
- Should date-stamp hypothesis context

**Owner**: Engineering (mitigation implementation)

**Flexibility**: HARD constraint on LLM; mitigatable via web search

**Timeline Impact**:
- Web search integration required in MVP (not stretch goal)
- Adds latency to hypothesis validation

**Resolution Path**:
1. Integrate web search for novelty checking
2. Query recent publications via web search
3. Clearly disclose knowledge cutoff in output
4. Timestamp when hypothesis was generated

---

### [SIG-002] Paper API Rate Limits Constrain Bulk Retrieval

**Category**: Technical
**Severity**: SIGNIFICANT

**Description**:
External paper APIs have rate limits that constrain bulk retrieval:

| API | Rate Limit | Coverage |
|-----|-----------|----------|
| **Semantic Scholar** | 100 requests/5 min (unauthenticated), 1 req/sec (authenticated) | 200M+ papers |
| **arXiv** | No strict limit, but be reasonable (1 req/3 sec recommended) | 2M+ papers (ML, physics, math) |
| **PubMed/NCBI** | 3 requests/sec (with API key), 10/sec (with NCBI account) | Biomedical focus |
| **OpenAlex** | 100K requests/day, 10 req/sec | 250M+ works |

**Hard Requirements**:
- Must implement rate limiting in evidence-gatherer
- Cannot do exhaustive paper searches; must be selective
- Need caching strategy for repeated queries

**Owner**: Engineering (implementation)

**Flexibility**: Can be mitigated with API keys, caching, and selective querying

**Timeline Impact**:
- Rate limiting adds implementation complexity
- May slow real-time retrieval; consider async/batch patterns

**Resolution Path**:
1. For MVP: Use web search only (no paper API integration)
2. Implement request queuing with backoff
3. Cache paper metadata locally
4. Batch similar queries where possible

---

### [SIG-003] Hypothesis Quality Bar Undefined

**Category**: Business
**Severity**: SIGNIFICANT

**Description**:
Strategy Document sets target of >40% expert validation rate, but does not define:
- What makes a hypothesis valid vs invalid
- How to operationalize quality measurement
- Minimum bar for MVP output

Without defined quality criteria, cannot evaluate if system is working.

**Hard Requirements**:
- Define hypothesis quality rubric before implementation
- Establish minimum bar for MVP demonstration
- Create mechanism for quality feedback

**Owner**: Product team

**Flexibility**: Must be defined, but specific criteria are negotiable

**Timeline Impact**:
- BLOCKS: Cannot evaluate challenger agent without quality criteria
- Should be resolved in architecture phase

**Resolution Path**:
1. Define hypothesis quality dimensions:
   - Specificity (testable as written?)
   - Novelty (not obviously published?)
   - Connection validity (genuine analogy, not superficial?)
   - Feasibility (could be tested with reasonable resources?)
2. Create scoring rubric (1-5 or pass/fail per dimension)
3. Minimum MVP bar: 3/5 average across dimensions

**Gap Reference**: See GAP-001

---

### [SIG-004] MVP Timeline Constrains Scope Severely

**Category**: Business
**Severity**: SIGNIFICANT

**Description**:
Strategy handoff specifies MVP scope: prototype demonstration within single session (~8 hours). This constrains:
- Cannot build full 6-agent constellation
- Cannot implement persistence
- Cannot integrate all paper APIs
- Cannot build UI

**Hard Requirements**:
- Working end-to-end demonstration
- At least one complete hypothesis generation
- Traceability to sources

**Owner**: Project lead

**Flexibility**: Scope can be negotiated, timeline is HARD

**Timeline Impact**:
- 8-hour constraint is binding
- Must ruthlessly prioritize core path

**Resolution Path**:
1. Identify critical path: input -> domain analysis -> cross-domain search -> hypothesis -> validation
2. Cut agents that are not on critical path for MVP
3. Defer: persistence, UI, multiple domain support, sophisticated scoring
4. Target: 1-2 working examples, not production system

---

### [SIG-005] Cross-Domain Synthesis Quality is Unproven

**Category**: Technical / Research
**Severity**: SIGNIFICANT

**Description**:
Core assumption in Strategy Document: LLMs can perform genuine cross-domain synthesis.

This is marked [ASSUMPTION: LLMs can perform genuine synthesis] with MEDIUM confidence. If false, product does not work.

**Hard Requirements**:
- Must validate synthesis capability early
- Need explicit test: can Claude identify non-obvious cross-domain connection that experts validate?
- Failure mode: superficial keyword matching disguised as synthesis

**Owner**: Technical Lead (validation)

**Flexibility**: HARD - if synthesis does not work, product is not viable

**Timeline Impact**:
- Should validate within first 2 hours of implementation
- Pivot or stop if core capability does not exist

**Resolution Path**:
1. Design explicit synthesis test case
2. Have known cross-domain connection as validation target
3. Test early, before building full pipeline
4. Define minimum synthesis quality to proceed

---

### [SIG-006] No Persistent Storage in MVP

**Category**: Technical
**Severity**: SIGNIFICANT

**Description**:
Project context specifies: No persistent database in MVP. This means:
- Domain knowledge recomputed each session
- No learning from past hypotheses
- No user preference storage
- Session state lost on exit

**Hard Requirements**:
- All state must fit in memory
- Context must be reconstructible from inputs
- Cannot cache expensive computations across sessions

**Owner**: Architecture decision

**Flexibility**: HARD for MVP, deferrable to v2

**Timeline Impact**:
- Simplifies implementation (no DB setup)
- May increase latency (recompute vs retrieve)

**Resolution Path**:
1. Design stateless hypothesis generation
2. Accept recomputation cost
3. Optionally: write results to JSON files for manual persistence

---

## Notable Constraints

Document and monitor these.

### [NOTE-001] TypeScript Strict Mode Required

**Category**: Technical
**Severity**: NOTABLE

**Description**: Per DECISION-002 and global coding standards, TypeScript strict mode is required.

**Requirements**:
- strict: true in tsconfig.json
- Explicit return types on public functions
- No as any type assertions
- Use discriminated unions over optional fields

**Owner**: Engineering standards

**Timeline Impact**: Adds ~10% implementation time for type annotations

---

### [NOTE-002] Initial Domains Fixed for MVP

**Category**: Business
**Severity**: NOTABLE

**Description**: Strategy Document specifies initial domains:
- Computational biology
- Materials science
- ML/AI

**Owner**: Product strategy (negotiable with product lead)

**Flexibility**: NEGOTIABLE - could reduce to 2 domains for MVP

**Timeline Impact**: Each additional domain adds validation complexity

---

### [NOTE-003] CLI-Only Interface for MVP

**Category**: Platform
**Severity**: NOTABLE

**Description**: Project context specifies CLI environment. No web UI or API server.

**Requirements**:
- Command-line input/output
- No browser dependencies
- Text-based formatting for results

**Owner**: Project scope decision

**Flexibility**: HARD for MVP

---

### [NOTE-004] ATLAS Team Pattern Compliance

**Category**: Organizational
**Severity**: NOTABLE

**Description**: Per DECISION-001, agent constellation must follow ATLAS team pattern:
- Planner -> Guardian -> Experts -> Spec-Writer -> Integrator flow
- Context sharing requirements
- Interface agent for external communication

**Owner**: Architecture (committed decision)

**Flexibility**: HARD - already decided

**Timeline Impact**: Pattern adds structure overhead but ensures quality

---

## Dependencies

### [DEP-001] Claude API Access

**Type**: Technical
**Owner**: Anthropic / Account holder

**Description**: Core dependency. Without Claude API access, nothing works.

**Status**: ASSUMED AVAILABLE
**Delivery Date**: N/A (external service)
**Confidence**: HIGH (if API key is valid)

**Risk Level**: LOW (mature service)

**Mitigation**: Ensure API key is valid before starting implementation.

**Tracking**: Test API connectivity as first implementation step.

---

### [DEP-002] Web Search Capability

**Type**: Technical
**Owner**: Engineering

**Description**: Required for:
- Novelty verification
- Recent publication discovery
- Citation validation

**Status**: TO BE IMPLEMENTED
**Delivery Date**: Within MVP timeline
**Confidence**: HIGH (standard capability)

**Risk Level**: LOW

**Mitigation**: Built-in web search tool available in Claude environment.

---

## Conflicts

### [CONFLICT-001] Timeline vs. Citation Rigor

**Severity**: SIGNIFICANT

**Description**:
8-hour MVP timeline conflicts with rigorous citation verification requirement.
Full citation verification for every claim would exceed time budget.

**Constraints in Conflict**:
- MVP Timeline: 8 hours total
- Citation Traceability: All connections must be traceable to sources

**Resolution Options**:

| Option | Description | Trade-off | Recommended |
|--------|-------------|-----------|-------------|
| **Reduce citation depth** | Single citation per hypothesis instead of comprehensive | Less rigorous, may miss important sources | Yes - for MVP |
| **Reduce hypothesis count** | Generate fewer, better-cited hypotheses | Less demonstration of capability | Maybe |
| **Accept unverified citations** | Trust LLM citations without verification | Violates core requirement | No |
| **Extend timeline** | More time for verification | Violates timeline constraint | No |

**Recommendation**: Reduce citation depth for MVP. Single verifiable citation per hypothesis, with note that full citation would be done in production.

**Escalation Required**: Product decision
**Decision Deadline**: Before implementation starts

---

## Gaps

Gaps are missing information that blocks planning.

### [GAP-001] Hypothesis Quality Rubric Undefined

**Blocking**: Yes

**Description**:
Strategy requires >40% expert validation rate, but no rubric exists to define what valid means.

**Impact**: Cannot evaluate challenger agent, cannot know if system works.

**Resolution**:
- **Action**: Define quality rubric with 4-5 dimensions
- **Owner**: Product lead
- **Deadline**: Before challenger agent design

**Proposed Rubric** (for discussion):

| Dimension | Pass Criteria |
|-----------|---------------|
| **Specificity** | Hypothesis can be tested as written (has variables, conditions) |
| **Novelty** | Web search does not return direct match |
| **Connection Validity** | Cross-domain mapping is semantic, not superficial |
| **Feasibility** | Could be tested with < USD 100K and < 1 year |
| **Grounding** | At least one real source supports the connection |

---

### [GAP-002] Domain Representation Schema Undecided

**Blocking**: Yes (for implementation)

**Description**:
OPEN-001 in context: How to represent domains and concepts is unresolved.

**Options**:
1. Flat tags (simple, MVP-friendly)
2. Hierarchical taxonomy (structured)
3. Concept graph (flexible, complex)
4. Embeddings (semantic, expensive)

**Impact**: Cannot implement domain-analyst without this.

**Resolution**:
- **Action**: Decide on flat tags for MVP
- **Owner**: Architecture phase
- **Deadline**: Before agent implementation

**Recommendation**: Flat tags for MVP. String labels like computational-biology, reinforcement-learning. Upgrade to graph in v2.

---

### [GAP-003] Cross-Domain Similarity Metric Undecided

**Blocking**: Yes (for cross-pollinator)

**Description**:
OPEN-002 in context: How to measure if two concepts are analogous enough for useful cross-pollination.

**Impact**: Core to cross-pollinator agent. Cannot proceed without decision.

**Resolution**:
- **Action**: Use LLM-based similarity for MVP
- **Owner**: Architecture phase
- **Deadline**: Before cross-pollinator implementation

**Recommendation**: LLM-based rating (ask Claude to score analogy quality 1-5). Fast to implement, good enough for MVP. Add embeddings in v2.

---

### [GAP-004] API Cost Budget Unspecified

**Blocking**: No (but risk)

**Description**:
No explicit budget for API costs. MVP could consume significant tokens:
- Domain analysis: ~10K tokens per domain
- Cross-domain search: ~20K tokens per search
- Synthesis: ~30K tokens per hypothesis
- Validation: ~10K tokens per hypothesis

Estimated MVP cost: USD 5-20 (Sonnet) or USD 25-100 (Opus)

**Impact**: May hit budget constraints without knowing them.

**Resolution**:
- **Action**: Set explicit API budget for MVP
- **Owner**: Project lead
- **Deadline**: Before implementation

**Recommendation**: Budget USD 50 for MVP. Use Sonnet for most agents, Opus only for synthesis.

---

## Quality Gates

What makes output acceptable.

### Hypothesis Quality Gate

A generated hypothesis is acceptable if it passes:

| Gate | Criteria | Failure Action |
|------|----------|----------------|
| **Specificity** | Contains testable claim (variables, conditions) | Reject, request refinement |
| **Citation** | At least one verifiable source | Reject, flag as ungrounded |
| **Non-Superficial** | Cross-domain connection is semantic, not keyword | Reject, log as false positive |
| **Not Published** | Web search does not return direct match | Flag as potentially known |
| **Feasibility** | Expert could design test | Flag if obviously untestable |

### System Quality Gate

MVP is acceptable if:

| Requirement | Metric | Minimum |
|-------------|--------|---------|
| End-to-end completion | Produces hypothesis from input | 1 working example |
| Citation traceability | Sources verifiable | 100% of claims |
| Cross-domain evidence | Connects 2+ domains | Every hypothesis |
| Specificity | Testable as written | >80% of hypotheses |
| Runtime | Completes in reasonable time | < 5 minutes per hypothesis |

---

## Non-Negotiables

These constraints cannot be compromised under any circumstances.

### 1. Citation Traceability
Every cross-domain claim must cite at least one verifiable source. No exceptions.
**Why**: Academic integrity, core value proposition.

### 2. No Fabricated Sources
Citations must be real. System cannot hallucinate paper titles, authors, or findings.
**Why**: Research ethics, trust, legal liability.

### 3. Transparent AI Generation
Output must clearly indicate it was AI-generated. Cannot be passed off as human research.
**Why**: Academic integrity, research ethics.

### 4. No Encouragement of Fraud
System cannot help users fabricate research or manufacture false claims.
**Why**: Research ethics, legal liability.

### 5. Type Safety
TypeScript strict mode, no as any, explicit types on public functions.
**Why**: Code quality standards, maintainability.

---

## Assumptions

What we are assuming is true. If false, reassess.

| Assumption | Confidence | Validation Method | Impact if False |
|------------|------------|-------------------|-----------------|
| LLMs can perform genuine cross-domain synthesis | MEDIUM | Early validation test | Product not viable |
| Researchers find AI hypothesis generation valuable | MEDIUM | User feedback | Market not viable |
| 200K context is sufficient for meaningful synthesis | MEDIUM | Test with real examples | Architecture redesign |
| Web search is sufficient for novelty checking | LOW | Compare to paper DB results | May need paper API |
| Flat domain tags are sufficient for MVP | HIGH | Test cross-domain matching | Minor refactor |
| Single citation per hypothesis is acceptable for MVP | HIGH | User feedback | Add citation depth |
| 8-hour timeline is achievable | MEDIUM | Time tracking | Scope reduction |
| API costs will be < USD 50 for MVP | MEDIUM | Cost monitoring | Budget increase |

---

## PM Guidance

Include this in feature breakdown and user stories.

### All Features

CONSTRAINTS SUMMARY:
- Context window: 200K tokens max (design for tiered loading)
- Citation required: Every cross-domain claim needs verifiable source
- Timeline: 8-hour MVP budget
- Storage: No persistence, in-memory only
- Interface: CLI only

QUALITY GATES:
- Hypotheses must be specific, cited, non-superficial, novel, feasible
- 100% citation traceability required

OPEN DECISIONS (resolve before implementation):
- Hypothesis quality rubric
- Domain representation schema
- Cross-domain similarity metric
- API cost budget

### Domain Analysis Features

CONSTRAINTS:
- Must extract: concepts, methods, open problems
- Output must be tokenizable within budget
- No external API calls (use LLM knowledge + web search)

DEPENDENCIES:
- Domain representation schema (GAP-002)

### Cross-Domain Search Features

CONSTRAINTS:
- Must use LLM-based similarity (MVP decision)
- Context budget: ~20K tokens per search
- Must log reasoning for traceability

DEPENDENCIES:
- Domain analysis output
- Similarity metric decision (GAP-003)

### Hypothesis Generation Features

CONSTRAINTS:
- Must include at least one citation
- Must be specific enough to test
- Context budget: ~30K tokens per synthesis

NON-NEGOTIABLES:
- No fabricated sources
- Transparent AI generation

### Validation/Challenger Features

CONSTRAINTS:
- Must check: specificity, novelty, connection validity, feasibility
- Web search required for novelty check
- Cannot approve hypothesis without citation

DEPENDENCIES:
- Hypothesis quality rubric (GAP-001)

---

## Verification Status

### Tier 1: Presence Verification
- [X] Technical constraints documented (context window, rate limits, costs)
- [X] Business constraints documented (timeline, scope, citation)
- [X] Legal/compliance constraints documented (academic integrity)
- [X] Platform constraints documented (TypeScript, CLI)
- [X] Quality constraints documented (gates defined)

### Tier 2: Content Verification
- [X] Context window limits verified against API docs
- [X] Rate limits verified against API docs
- [X] Timeline constraint verified against project context
- [X] Citation requirement verified against strategy doc
- [X] Quality criteria extracted from strategy

### Tier 3: Authority Verification
- [ ] API budget confirmed with project lead
- [ ] Hypothesis rubric approved by product lead
- [ ] Domain schema decision confirmed

### Analysis Scope

```yaml
analysis_scope:
  constraint_categories_reviewed: 5
  total_constraints_documented: 12
  verification_tier_distribution:
    tier_1: 4   # Presence verified
    tier_2: 6   # Content verified
    tier_3: 2   # Pending owner confirmation
  time_budget_used: "45 minutes"
  unexplored_areas:
    - "Institutional compliance (not relevant for personal MVP)"
    - "Data retention requirements (no user data in MVP)"
  confidence_impact: "High confidence in documented constraints. Medium risk in GAP items."
```

---

## Guardian Handoff

```yaml
guardian_handoff:
  ccd_reference: "docs/specs/CONSTRAINTS.md"
  
  blocking_items:
    - item: "Context window design for cross-domain synthesis"
      resolution_owner: "Architecture phase"
      decision_deadline: "Before agent implementation"
    
    - item: "Citation traceability implementation"
      resolution_owner: "Engineering"
      decision_deadline: "Core to all agents"
  
  pm_actions_required:
    - action: "Define hypothesis quality rubric"
      deadline: "Before challenger agent design"
    
    - action: "Confirm API cost budget"
      deadline: "Before implementation"
    
    - action: "Decide on domain representation (recommend: flat tags)"
      deadline: "Before domain-analyst implementation"
    
    - action: "Decide on similarity metric (recommend: LLM-based)"
      deadline: "Before cross-pollinator implementation"
  
  constraints_for_stories:
    - constraint: "200K context window"
      affects: "All synthesis features"
      guidance: "Design for tiered context loading"
    
    - constraint: "Citation required"
      affects: "All hypothesis output"
      guidance: "Every claim needs verifiable source"
    
    - constraint: "8-hour timeline"
      affects: "Scope decisions"
      guidance: "Ruthlessly prioritize critical path"
  
  escalations_in_progress:
    - topic: "Timeline vs citation rigor trade-off"
      decision_owner: "Product lead"
      expected_resolution: "Before implementation"
  
  verification_gaps:
    - gap: "API budget unconfirmed"
      risk: "MEDIUM"
      recommendation: "Set USD 50 budget, monitor costs"
    
    - gap: "Cross-domain synthesis capability unproven"
      risk: "HIGH"
      recommendation: "Validate within first 2 hours"
```

---

*Generated by project-guardian following Constraint-First Planning Protocol (CFPP)*
*Confidence: MEDIUM - Core constraints documented with explicit gaps. Validation pending for research feasibility assumption.*
